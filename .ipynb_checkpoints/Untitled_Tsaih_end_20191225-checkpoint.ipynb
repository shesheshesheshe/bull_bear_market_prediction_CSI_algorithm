{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "filename = \"tsaih_train_all.csv\"\n",
    "x_data = []\n",
    "y_data = []\n",
    "with open(filename) as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    first = True\n",
    "    for row in spamreader:\n",
    "        if first:\n",
    "            first = False\n",
    "            continue\n",
    "        x_datas = row[:-1]\n",
    "        x_datas = [float(f) for f in x_datas]\n",
    "        y_datas = row[len(x_datas):]\n",
    "        y_datas = [float(f) for f in y_datas]\n",
    "        x_data.append(x_datas)\n",
    "        y_data.append(y_datas)\n",
    "x_data = np.array(x_data, dtype = np.float32)\n",
    "y_data = np.array(y_data, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input node, m = 12\n",
      "Number of hidden node, p = 1\n",
      "Number of inputs, q = 479\n"
     ]
    }
   ],
   "source": [
    "input_num, input_node = np.array(x_data).shape\n",
    "hidden_node = 1\n",
    "output_node = 1\n",
    "LearningRate = 0.01   \n",
    "print(\"Number of input node, m =\", input_node)\n",
    "print(\"Number of hidden node, p =\", hidden_node)\n",
    "print(\"Number of inputs, q =\", input_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Variables & Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "RegularizationTerm = 0.001\n",
    "\n",
    "temp_hw = 0\n",
    "temp_ht = 0\n",
    "temp_ho = 0\n",
    "temp_oo = 0\n",
    "temp_ow = 0\n",
    "temp_ot = 0\n",
    "temp_error = 0\n",
    "\n",
    "#print(\"Round:\", n, \"\\nInitial model.\")\n",
    "#n = n + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init = tf.global_variables_initializer()\n",
    "\n",
    "def add_layer(inputs, input_size, output_size, weights = None, threshold = None, activation_function = None):\n",
    "    if weights is None:\n",
    "        weights = tf.Variable(tf.random_normal([input_size, output_size]))\n",
    "    if threshold is None:\n",
    "        threshold = tf.Variable(tf.zeros([1, output_size]) + 0.1)\n",
    "    Wx_plus_b = tf.matmul(inputs, weights) + threshold\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "    return outputs, weights, threshold\n",
    "\n",
    "#符合即可繼續學習\n",
    "def LearningRate():\n",
    "    return learning_rate >= 0.01\n",
    "\n",
    "def f_x_w():\n",
    "    result = tf.reduce_sum(tf.square(y_data - oo))\n",
    "    return result\n",
    "\n",
    "#符合即需停止學習，學習完成\n",
    "def lts_stopping():\n",
    "    return n > N\n",
    "\n",
    "def lts():\n",
    "    n = n + 1\n",
    "    lts_x_data = []\n",
    "    lts_y_data = []\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        residual = sess.run(tf.square(y_data - oo))\n",
    "    for i in range(n):\n",
    "        index = np.where(residual == residual.min())\n",
    "        #index_list.append(index[0])\n",
    "        residual[index[0]] = residual.max() + 1\n",
    "        lts_x_data.extend(x_data[index[0]])\n",
    "        lts_y_data.extend(y_data[index[0]])\n",
    "    lts_x_data = tf.convert_to_tensor(lts_x_data)\n",
    "    lts_y_data = tf.convert_to_tensor(lts_y_data)\n",
    "    print(\"Round:\", n)\n",
    "    #print(lts_x_data, lts_y_data)\n",
    "\n",
    "def train():\n",
    "#    ho, hw, ht = add_layer(lts_x_data, input_node, hidden_node, weights = None, threshold = None, activation_function = tf.tanh)\n",
    "#    oo, ow, ot = add_layer(ho, hidden_node, output_node, weights = None, threshold = None, activation_function = None)\n",
    "    LossFunction = tf.Variable(tf.reduce_mean(tf.reduce_sum(tf.square(lts_y_data - oo), reduction_indices=[1]))\n",
    "                               + tf.math.multiply(RegularizationTerm, tf.reduce_sum(tf.square(ow), reduction_indices=[1])\n",
    "                                                  + tf.reduce_sum(tf.square(hw), reduction_indices=[1])\n",
    "                                                  + tf.reduce_sum(tf.square(ht), reduction_indices=[1])\n",
    "                                                  + tf.reduce_sum(tf.square(ot), reduction_indices=[1]))\n",
    "                              )\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:        \n",
    "        tf.global_variables_initializer().run()\n",
    "        error = sess.run(LossFunction)\n",
    "        sess.run(tf.train.GradientDescentOptimizer(learning_rate).minimize(error))\n",
    "        \n",
    "\n",
    "def initial_SLFN():\n",
    "    n = input_node + 1\n",
    "    lts_x_data = tf.convert_to_tensor(x_data[0:n])\n",
    "    lts_y_data = tf.convert_to_tensor(y_data[0:n])\n",
    "    hw = tf.Variable(([3.813612998], [-6.785246965], [4.303358605], [0.024616483], [469.434092], [-479.7924935], [10.52539191], [-1.329120088], [632.6970342], [-95.56595298], [-3.906295076], [-79.98295145]))\n",
    "    ht = tf.Variable(([[698.052694]]))\n",
    "    ow = tf.Variable(([[4.0]]))\n",
    "    ot = tf.Variable(([[-2.0]]))\n",
    "    error = weight_tunning()\n",
    "\n",
    "#符合即學成(待修改)\n",
    "def conditionL():\n",
    "    a = lts_y_data[lts_y_data < 0].max()\n",
    "    b = lts_y_data[lts_y_data > 0].min()\n",
    "    print(a, b)\n",
    "    return a < b\n",
    "\n",
    "def save():\n",
    "    temp_hw = hw\n",
    "    temp_ht = ht\n",
    "    temp_ho = ho\n",
    "    temp_oo = oo\n",
    "    temp_ow = ow\n",
    "    temp_ot = ot\n",
    "    temp_error = error\n",
    "\n",
    "#(待修改)\n",
    "#def weight_tunning():\n",
    "#    train()\n",
    "#    while LearningRate():\n",
    "#        if error < temp_error:\n",
    "            #restore w\n",
    "            #cramming\n",
    "#        else:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lts_y_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-bf06ad5b8e3b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#    ho, hw, ht = add_layer(lts_x_data, input_node, hidden_node, weights = None, threshold = None, activation_function = tf.tanh)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m#    oo, ow, ot = add_layer(ho, hidden_node, output_node, weights = None, threshold = None, activation_function = None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     LossFunction = tf.Variable(tf.reduce_mean(tf.reduce_sum(tf.square(lts_y_data - oo), reduction_indices=[1]))\n\u001b[0m\u001b[1;32m     49\u001b[0m                                + tf.math.multiply(RegularizationTerm, tf.reduce_sum(tf.square(ow), reduction_indices=[1])\n\u001b[1;32m     50\u001b[0m                                                   \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lts_y_data' is not defined"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "initial_SLFN()\n",
    "\n",
    "while lts_stopping() == False:\n",
    "    lts()\n",
    "    if conditionL():\n",
    "        #Softening\n",
    "    else:\n",
    "        save()\n",
    "        weight_tunning()\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "sess.close()\n",
    "\n",
    "#print(lts_y_data.shape, oo.shape)\n",
    "#print(x_data.shape, hw.shape, ht.shape)\n",
    "#print(oo)\n",
    "#print(ho.shape, ow.shape, ot.shape)\n",
    "#print(ho.shape, hw.shape, ht.shape)\n",
    "#print(type(ho), type(hw), type(ht), sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.Session() as sess:        \n",
    "    tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Step1.1 Initial SLFN model\n",
    "sess.run(train, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "Output = sess.run(OutputLOutput, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "Error = sess.run(LossFunction, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "iniHWeights = sess.run(HWeights, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "iniHThreshold = sess.run(HThreshold, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "iniOWeights = sess.run(OWeights, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "iniOThreshold = sess.run(OThreshold, feed_dict={xs: lts_x_data, yc: lts_y_data}) \n",
    "\n",
    "#Step2. Add another case into I(n)\n",
    "  #Stopping Criteria1: n = dataVolume\n",
    "while n <= dataVolume:\n",
    "    #Save previous error, weights & thresholds\n",
    "    preError = Error\n",
    "    preHWeights = sess.run(HWeights, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "    preHThreshold = sess.run(HThreshold, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "    preOWeights = sess.run(OWeights, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "    preOThreshold = sess.run(OThreshold, feed_dict={xs: lts_x_data, yc: lts_y_data})    \n",
    "    lts_x_error = sess.run(residual, feed_dict={xs: x_data, yc: y_data})\n",
    "    lts_x_data, lts_y_data = lts(n, lts_x_error, x_data, y_data)\n",
    "    #debugging\n",
    "    print(\"Round:\", n)\n",
    "    #print(lts_x_data)\n",
    "    #print(lts_y_data)\n",
    "    #print(preError, preHThreshold, preHWeights, preOThreshold, preOWeights, sep = \"\\n\")\n",
    "    n = n + 1\n",
    "\n",
    "#Step3. Whether I(n) satisfying the Learning Goal?\n",
    "    LearningGoal = sess.run(LearningGoalFunction, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "    #Stopping Criteria2: LearningGoal <= 0.1\n",
    "    if LearningGoal <= 0.1:\n",
    "        print(\"No out-lier.\")\n",
    "        continue\n",
    "        \n",
    "#Step4. Weight-Tuning Mechanism: Creating a new SLFN model\n",
    "    else:\n",
    "        print(\"Out-lier detection!\")\n",
    "        time = 0\n",
    "        #Step5-1. Stopping Criteria(1): LearningGoal <= 0.1\n",
    "        while LearningGoal > 0.1:\n",
    "            time = time + 1\n",
    "            #train a new model\n",
    "            sess.run(train, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "            Output = sess.run(OutputLOutput, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "            if Error > preError:\n",
    "\n",
    "        #Step5-2. Stopping Criteria(2): Learning-rate\n",
    "                if LearningRate < 0.01:\n",
    "                    print(time, \"Fail: due to tiny learning rate(\", LearningRate, \")\")\n",
    "                    #restore weights & thresholds (Step.6 Cramming & Softening Mechanism... to be updated)\n",
    "                    #sess.run(tf.assign(HWeights, iniHWeights))\n",
    "                    #sess.run(tf.assign(HThreshold, iniHThreshold))\n",
    "                    #sess.run(tf.assign(OWeights, iniOWeights))\n",
    "                    #sess.run(tf.assign(OThreshold, iniOThreshold)) \n",
    "                    break\n",
    "                else:\n",
    "                    print(time, \"Keep learning at learning rate =\", LearningRate)\n",
    "                    LearningRate = LearningRate * 0.7\n",
    "                    sess.run(tf.assign(HWeights, preHWeights))\n",
    "                    sess.run(tf.assign(HThreshold, preHThreshold))\n",
    "                    sess.run(tf.assign(OWeights, preOWeights))\n",
    "                    sess.run(tf.assign(OThreshold, preOThreshold)) \n",
    "\n",
    "            else:\n",
    "                print(time, \"Keep learning at learning rate =\", LearningRate)\n",
    "                LearningRate = LearningRate * 1.2\n",
    "                Error = sess.run(LossFunction, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "        print(\"Stop, Learning Goal =\", LearningGoal)\n",
    "        sess.run(train, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "        Output = sess.run(OutputLOutput, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "        Error = sess.run(LossFunction, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "        LearningGoal = sess.run(LearningGoalFunction, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "        print(Output)\n",
    "        \n",
    "            #debugging\n",
    "        #Weights = sess.run(OWeights, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "        #LearningGoal = sess.run(LearningGoalFunction, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "        #LearningRateArr.append(LearningRate)\n",
    "        #ErrorArr.append(Error)\n",
    "        #print(\"Time =\", time, \" Learning Rate =\", LearningRate, \" Error =\", Error)\n",
    "        #print(\"Weights =\", Weights)\n",
    "        #print(\"Output =\", Output)\n",
    "        #LearningRateGraph = plt.plot(LearningRateArr)\n",
    "        #print(\"Learning Rate\", LearningRateGraph)\n",
    "        #ErrorGraph = plt.plot(ErrorArr)\n",
    "        #print(\"Error\", ErrorGraph)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def learning_goal(y_data, output):\n",
    "    LearningGoal = 0.1\n",
    "    LearningGoalFunction = tf.reduce_mean(tf.reduce_sum(tf.math.abs(y_data - output), reduction_indices=[1]))\n",
    "    return LearningGoalFunction < LearningGoal\n",
    "\n",
    "def cramming(input_node, hidden_node, n, x_data, y_data, output):\n",
    "    tau = 1.1\n",
    "    zeta = 0.003\n",
    "    gama = tf.Variable(tf.random_normal([1, input_node]))\n",
    "    c_y_data = y_data[:n]\n",
    "    c_output = output[:n]\n",
    "    while (tf.transpose(gama) * (c_y_data - c_output) == 0) and (tf.math.multiply(zeta, tf.math.multiply(tf.transpose(gama), (c_y_data - c_output))) >= 0):\n",
    "        gama = tf.Variable(tf.random_normal([1, input_node]))\n",
    "        \n",
    "    gamat = tf.transpose(gama)\n",
    "    hidden_node = hidden_node + 3\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#def softening_integrating():\n",
    "\n",
    "def SLFN(inputSize, hiddenSize, outputSize\n",
    "         , x_data, y_data, LearningRate\n",
    "         , temp_oo, temp_ow, temp_hw, temp_ot, temp_ht):\n",
    "    if (temp_hw == None):\n",
    "        ho, hw, ht = add_layer(x_data, inputSize, hiddenSize, weights = None, threshold = None, activation_function = tf.tanh)\n",
    "        oo, ow, ot = add_layer(ho, hiddenSize, outputSize, weights = None, threshold = None, activation_function = None)\n",
    "        temp_error, temp_ow, temp_hw, temp_ot, temp_ht = weight_tunning(y_data, oo, ow, hw, ot, ht, LearningRate)\n",
    "        return error, oo, temp_ow, temp_hw, temp_ot, temp_ht\n",
    "    else:\n",
    "        error, output, ow, hw, ot, ht = weight_tunning(y_data, temp_oo, temp_ow, temp_hw, temp_ot, temp_ht, LearningRate)    \n",
    "        return error, output, ow, hw, ot, ht\n",
    "\n",
    "#def debugging():\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go to Terminal\n",
    "#enter:\n",
    "    #cd ~user\n",
    "    #tensorboard --logdir=showmethetensorboard --host=127.0.0.1\n",
    "#Copy the URL to your browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_data[y_data==1]=0\n",
    "#y_data[np.random.randint()]\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
