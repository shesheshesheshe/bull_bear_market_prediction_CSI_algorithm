{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "filename = \"tsaih_train_all.csv\"\n",
    "x_data = []\n",
    "y_data = []\n",
    "with open(filename) as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    first = True\n",
    "    for row in spamreader:\n",
    "        if first:\n",
    "            first = False\n",
    "            continue\n",
    "        x_datas = row[:-1]\n",
    "        x_datas = [float(f) for f in x_datas]\n",
    "        y_datas = row[len(x_datas):]\n",
    "        y_datas = [float(f) for f in y_datas]\n",
    "        x_data.append(x_datas)\n",
    "        y_data.append(y_datas)\n",
    "x_data = np.array(x_data, dtype = np.float32)\n",
    "y_data = np.array(y_data, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input node, m = 12\n",
      "Number of hidden node, p = 1\n",
      "Number of inputs, q = 479\n"
     ]
    }
   ],
   "source": [
    "input_num, input_node = np.array(x_data).shape\n",
    "hidden_node = 1\n",
    "output_node = 1\n",
    "LearningRate = 0.01   \n",
    "print(\"Number of input node, m =\", input_node)\n",
    "print(\"Number of hidden node, p =\", hidden_node)\n",
    "print(\"Number of inputs, q =\", input_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "print(\"Round:\", n, \"\\nInitial model.\")\n",
    "n = n + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hlayer(inputs, input_size, output_size, weights = None, threshold = None):\n",
    "    if weights is None:\n",
    "        weights = tf.Variable(tf.random_normal([input_size, output_size]))\n",
    "    if threshold is None:\n",
    "        threshold = tf.Variable(tf.zeros([1, output_size]) + 0.1)\n",
    "    weights = np.reshape(weights, (input_size, output_size))\n",
    "    Wx_plus_b = tf.matmul(inputs, weights) + threshold\n",
    "    outputs = tf.tanh(Wx_plus_b)\n",
    "    return outputs, weights, threshold\n",
    "\n",
    "def add_olayer(inputs, input_size, output_size, weights = None, threshold = None):\n",
    "    if weights is None:\n",
    "        weights = tf.Variable(tf.random_normal([input_size, output_size]), dtype = np.float32)\n",
    "    if threshold is None:\n",
    "        threshold = tf.Variable(tf.zeros([1, output_size]) + 0.1, dtype = np.float32)\n",
    "    Wx_plus_b = tf.matmul(inputs, weights) + threshold\n",
    "    outputs = Wx_plus_b\n",
    "    return outputs, weights, threshold\n",
    "\n",
    "#符合即學成(待修改)\n",
    "def conditionL():\n",
    "    a = lts_y_data[lts_y_data < 0].max()\n",
    "    b = lts_y_data[lts_y_data > 0].min()\n",
    "    print(a, b)\n",
    "    return a < b\n",
    "\n",
    "#符合即需停止學習、學習失敗，進入 cramming\n",
    "def LearningRate():\n",
    "    return learning_rate < 0.01\n",
    "\n",
    "def f_x_w():\n",
    "    result = tf.reduce_sum(tf.square(y_data - oo))\n",
    "    return result\n",
    "\n",
    "def lts():\n",
    "    lts_x_data = []\n",
    "    lts_y_data = []\n",
    "    residual = np.atleast_1d(tf.Variable(tf.square(y_data - f_x_w())))\n",
    "    print(residual.shape)\n",
    "    #for i in range(n):\n",
    "        #index = np.where(tf.reduce_min(residual))\n",
    "        #print(index[0])\n",
    "        #residual[index[0]] = tf.reduce_min(residual) + 1\n",
    "        #lts_x_data.extend(x_data[index[0]])\n",
    "        #lts_y_data.extend(y_data[index[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "lts()\n",
    "#lts_x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1. Initial SLFN model\n",
    "n = input_node + 1\n",
    "lts_x_data = x_data[0:n]\n",
    "lts_y_data = y_data[0:n]\n",
    "hw = np.asarray([[3.813612998], [-6.785246965], [4.303358605], [0.024616483], [469.434092], [-479.7924935], [10.52539191], [-1.329120088], [632.6970342], [-95.56595298], [-3.906295076], [-79.98295145]], dtype = np.float32)\n",
    "ht = [[698.052694]]\n",
    "ow = [[4.0]]\n",
    "ot = [[-2.0]]\n",
    "ho, hw, ht = add_hlayer(x_data, input_node, hidden_node, weights = hw, threshold = ht)\n",
    "oo, ow, ot = add_olayer(ho, hidden_node, output_node, weights = ow, threshold = ot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def learning_goal(y_data, output):\n",
    "    LearningGoal = 0.1\n",
    "    LearningGoalFunction = tf.reduce_mean(tf.reduce_sum(tf.math.abs(y_data - output), reduction_indices=[1]))\n",
    "    return LearningGoalFunction < LearningGoal\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def weight_tunning(y_data, output, ow, hw, ot, ht, LearningRate):\n",
    "    RegularizationTerm = 0.001 \n",
    "    LossFunction = tf.reduce_mean(tf.reduce_sum(tf.square(y_data - output)\n",
    "                      , reduction_indices=[1])) + tf.math.multiply(RegularizationTerm\n",
    "                      , tf.reduce_sum(tf.square(ow)) + tf.reduce_sum(tf.square(hw)) + tf.reduce_sum(tf.square(ht)) + tf.reduce_sum(tf.square(ot)))\n",
    "    train = tf.train.GradientDescentOptimizer(LearningRate).minimize(LossFunction)\n",
    "    return LossFunction, ow, hw, ot, ht\n",
    "\n",
    "def cramming(input_node, hidden_node, n, x_data, y_data, output):\n",
    "    tau = 1.1\n",
    "    zeta = 0.003\n",
    "    gama = tf.Variable(tf.random_normal([1, input_node]))\n",
    "    c_y_data = y_data[:n]\n",
    "    c_output = output[:n]\n",
    "    while (tf.transpose(gama) * (c_y_data - c_output) == 0) and (tf.math.multiply(zeta, tf.math.multiply(tf.transpose(gama), (c_y_data - c_output))) >= 0):\n",
    "        gama = tf.Variable(tf.random_normal([1, input_node]))\n",
    "        \n",
    "    gamat = tf.transpose(gama)\n",
    "    hidden_node = hidden_node + 3\n",
    "    \n",
    "    \n",
    "#def softening_integrating():\n",
    "\n",
    "def SLFN(inputSize, hiddenSize, outputSize\n",
    "         , x_data, y_data, LearningRate\n",
    "         , temp_oo, temp_ow, temp_hw, temp_ot, temp_ht):\n",
    "    if (temp_hw == None):\n",
    "        ho, hw, ht = add_layer(x_data, inputSize, hiddenSize, weights = None, threshold = None, activation_function = tf.tanh)\n",
    "        oo, ow, ot = add_layer(ho, hiddenSize, outputSize, weights = None, threshold = None, activation_function = None)\n",
    "        temp_error, temp_ow, temp_hw, temp_ot, temp_ht = weight_tunning(y_data, oo, ow, hw, ot, ht, LearningRate)\n",
    "        return error, oo, temp_ow, temp_hw, temp_ot, temp_ht\n",
    "    else:\n",
    "        error, output, ow, hw, ot, ht = weight_tunning(y_data, temp_oo, temp_ow, temp_hw, temp_ot, temp_ht, LearningRate)    \n",
    "        return error, output, ow, hw, ot, ht\n",
    "\n",
    "#def debugging():\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "#Step1. n++\n",
    "    #Stopping Criteria1, LTS: input_num < n\n",
    "while (input_num <= n):\n",
    "    error, output, ow, hw, ot, ht = SLFN(input_node, hidden_node, output_node\n",
    "                                             , lts_x_data, lts_y_data, LearningRate\n",
    "                                             , temp_oo = None, temp_ow = None, temp_hw = None, temp_ot = None, temp_ht = None)\n",
    "\n",
    "    #Add another case into I(n), part2/2\n",
    "    lts_x_data, lts_y_data = lts(y_data, output, n, x_data, lts_x_data, lts_y_data)\n",
    "    \n",
    "    #Save previous error, weights & thresholds\n",
    "    \n",
    "    print(\"Round:\", n)\n",
    "    \n",
    "#Step2. Weight-Tuning Mechanism: Creating a new SLFN model\n",
    "  \n",
    "    if (conditionL(temp_output, n)):\n",
    "        print(\"Time:\", time, \"\\nNo out-lier.\")\n",
    "        continue\n",
    "    else:\n",
    "        print(\"Time:\", time, \"\\nOut-lier detection!\")\n",
    "        time = 0  \n",
    "        while (conditionL(lts_y_data, input_num)):\n",
    "            time = time + 1\n",
    "            temp_error, temp_output, temp_ow, temp_hw, temp_ot, temp_ht = error, output, ow, hw, ot, ht            \n",
    "            error, output, ow, hw, ot, ht = SLFN(input_node, hidden_node, output_node\n",
    "                                             , lts_x_data, lts_y_data, LearningRate\n",
    "                                             , temp_oo = None, temp_ow = None, temp_hw = None, temp_ot = None, temp_ht = None)\n",
    "            if error > temp_error:\n",
    "                if LearningRate(learning_rate) == False:\n",
    "                    print(\"Cramming\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Slow down learning.\")\n",
    "                    learning_rate = learning_rate * 0.7\n",
    "                    \n",
    "                    error, oo, ow, hw, ot, ht = SLFN(input_node, hidden_node, output_node\n",
    "                                                     , lts_x_data, lts_y_data, LearningRate\n",
    "                                                     , temp_oo = temp_oo, temp_ow = temp_ow, temp_hw = temp_hw, temp_ot = temp_ot, temp_ht = None)\n",
    "            else:\n",
    "                learning_rate = learning_rate * 1.2\n",
    "                error, oo, ow, hw, ot, ht = SLFN(input_node, hidden_node, output_node\n",
    "                                             , lts_x_data, lts_y_data, LearningRate\n",
    "                                             , temp_oo = None, temp_ow = None, temp_hw = None, temp_ot = None, temp_ht = None)\n",
    "\n",
    "        n = n + 1\n",
    "        print(\"Softening & Integrating\")\n",
    "        \n",
    "        \n",
    "           #debugging\n",
    "        #Weights = sess.run(OWeights, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "        #LearningGoal = sess.run(LearningGoalFunction, feed_dict={xs: lts_x_data, yc: lts_y_data})\n",
    "        #LearningRateArr.append(LearningRate)\n",
    "        #ErrorArr.append(Error)\n",
    "        #print(\"Time =\", time, \" Learning Rate =\", LearningRate, \" Error =\", Error)\n",
    "        #print(\"Weights =\", Weights)\n",
    "        #print(\"Output =\", Output)\n",
    "        #LearningRateGraph = plt.plot(LearningRateArr)\n",
    "        #print(\"Learning Rate\", LearningRateGraph)\n",
    "        #ErrorGraph = plt.plot(ErrorArr)\n",
    "        #print(\"Error\", ErrorGraph)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go to Terminal\n",
    "#enter:\n",
    "    #cd ~user\n",
    "    #tensorboard --logdir=showmethetensorboard --host=127.0.0.1\n",
    "#Copy the URL to your browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_data[y_data==1]=0\n",
    "#y_data[np.random.randint()]\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 2, 3], [2, 3])\n",
      "[list([1, 2, 3]) list([2, 3])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = [1, 2, 3], [2, 3]\n",
    "print(g)\n",
    "print(np.transpose(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
